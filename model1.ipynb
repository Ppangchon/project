{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ppangchon/project/blob/main/model1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 1"
      ],
      "metadata": {
        "id": "34ai61h4sV3N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*** การกำหนดขนาดจากเปเปอร์ (https://www.agi.nu.ac.th/nred/Document/is-PDF/2562/geo_2562_05_FullPaper.pdf) ***"
      ],
      "metadata": {
        "id": "ijW4crD5siJN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "metadata": {
        "id": "WIvY-xgtsR_s"
      },
      "outputs": [],
      "source": [
        "import os,cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pathlib\n",
        "import PIL\n",
        "import plotly\n",
        "import plotly.graph_objs as go\n",
        "import pandas as pd\n",
        "from numpy import array\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import (Dense, Conv2D, AveragePooling2D, Flatten, Dropout, MaxPool2D)\n",
        "import time\n",
        "from keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.models import model_from_json\n",
        "from tensorflow.keras.models import Sequential\n",
        "import requests\n",
        "from IPython.display import Image\n",
        "from io import BytesIO  \n",
        "import pickle as p \n",
        "import plotly.graph_objs as go  \n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras import backend as K\n",
        "K.image_data_format()\n",
        "from keras.utils import np_utils"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## เตรียมชุดของข้อมูล"
      ],
      "metadata": {
        "id": "YR2KRg5yspJ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive # เชื่อม drive ของเรา ถ้าเชื่อมสำเร็จจะขึ้นคำว่า Mounted at /content/drive \n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGLTjcaRsgLR",
        "outputId": "1d3f95eb-ac64-48aa-c4de-98d0b6bba41f"
      },
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os # os จัดการเกี่ยวกับไฟล์ต่างๆ ในโฟล์เดอร์\n",
        "path = '/content/drive/My Drive/lastoct'  # data เราอยู่โฟล์เดอร์ไหน"
      ],
      "metadata": {
        "id": "rx-U9VJnIQLF"
      },
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32   # กำหนดขนาดของแต่ละการอ่านข้อมูล\n",
        "img_height = 256  # ความสูงของรูปภาพแต่ละรูป\n",
        "img_width = 256   # ความกว้างของรูปภาพแต่ละรูป\n",
        "\n",
        "num_classes = 4"
      ],
      "metadata": {
        "id": "YMRBOd5EsUkW"
      },
      "execution_count": 213,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "5w4H_gR_s1wd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train\n",
        "train_dir = \"/content/drive/My Drive/lastoct/train\"\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "train_set = train_datagen.flow_from_directory(train_dir, target_size=(img_height, img_width), batch_size=batch_size, class_mode='categorical')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W153meQLsVTR",
        "outputId": "bf3ef478-d1c3-4b8e-c8d1-5d18521dc0dd"
      },
      "execution_count": 226,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 17779 images belonging to 4 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Train')\n",
        "ids, counts = np.unique(train_set.classes, return_counts=True)\n",
        "print(ids)        # คลาส\n",
        "print(counts)     # จำนวนภาพของคลาส"
      ],
      "metadata": {
        "id": "-OKk6KZ3Ii96",
        "outputId": "d19bf6b1-68ea-4026-9921-53adc3f9f53f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 227,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train\n",
            "[0 1 2 3]\n",
            "[4552 4469 4385 4373]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_set[0][0][0].shape"
      ],
      "metadata": {
        "id": "7QNpzE4zG7WZ",
        "outputId": "cbf50733-5972-4a1c-c813-cbdb84000722",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 228,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(256, 256, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 228
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validation"
      ],
      "metadata": {
        "id": "njnfmC6BtNs8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation\n",
        "val_dir =\"/content/drive/My Drive/lastoct/validation\"\n",
        "val_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "val_set = val_datagen.flow_from_directory(val_dir, target_size=target_img_shape, batch_size=10, class_mode='categorical')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVZ4VO8dtOh9",
        "outputId": "b3bda334-127d-4da0-89cd-2771499a073f"
      },
      "execution_count": 229,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2217 images belonging to 4 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Validation')\n",
        "ids, counts = np.unique(val_set.classes, return_counts=True)\n",
        "print(ids)        # คลาส\n",
        "print(counts)     # จำนวนข้อมูลของคลาส"
      ],
      "metadata": {
        "id": "3b8KrVtiRL2q",
        "outputId": "b0e494c6-d433-4f3d-bbda-5f46f74a13bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 230,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation\n",
            "[0 1 2 3]\n",
            "[528 546 540 603]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test"
      ],
      "metadata": {
        "id": "MvAztmtyvHLv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation\n",
        "test_dir =\"/content/drive/My Drive/lastoct/test\"\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "test_set = tf.keras.utils.image_dataset_from_directory(test_dir,image_size=(img_height, img_width))"
      ],
      "metadata": {
        "id": "U_DoK8JWRidn",
        "outputId": "96d328a2-e18d-4c61-cdb2-9502fc9abd8b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 231,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2182 files belonging to 4 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = test_set.class_names\n",
        "print(class_names)"
      ],
      "metadata": {
        "id": "_uitJIbj99Yg",
        "outputId": "8548acba-f213-439c-ae1d-519da65c360c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 232,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['CSC', 'Normal', 'PCV', 'VKH']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "vznhXHvKwWko"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "Eop7u0fjwZdF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 4\n",
        "epochs=10   # จำนวนครั้งที่เราจะ Train\n",
        "# ขนาดภาพอินพุตเข้าสู่ Model\n",
        "in_shape = (256,256,3)\n",
        "in_shape"
      ],
      "metadata": {
        "id": "aYQTG-KdwV-G",
        "outputId": "f5cb9fa4-f289-44d4-c7ae-8d555c58527f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 233,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(256, 256, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 233
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()   # สร้าง Model\n",
        "\n",
        "# สร้างชั้น Convolution ชั้นแรก ค่าพารามิเตอร์ activation ใช้ ReLU\n",
        "model.add(Conv2D(32, (3,3), activation = 'relu', input_shape = in_shape))\n",
        "model.add(MaxPool2D((2,2)))    # สร้างชั้น Max Pooling ค่าพารามิเตอร์ 2,2 คือ Pool Size\n",
        "\n",
        "# ชั้นที่ 2\n",
        "model.add(Conv2D(64, (3,3), activation = 'relu'))\n",
        "model.add(MaxPool2D((2,2)))\n",
        "\n",
        "# ชั้นที่ 3 \n",
        "model.add(Conv2D(128, (3,3), activation='relu'))\n",
        "model.add(MaxPool2D((2,2)))\n",
        "\n",
        "# ชั้นที่ 4 (Convolution + Max Pooling)\n",
        "model.add(Conv2D(256, (3,3), activation='relu'))\n",
        "model.add(MaxPool2D((2,2)))\n",
        "\n",
        "# สร้างชั้น Neural Network \n",
        "model.add(Flatten())\n",
        "model.add(Dense(4, activation='softmax'))\n",
        "model.summary()          # ดูสรุปโครงสร้าง Model"
      ],
      "metadata": {
        "id": "WB3h7Ca8SW_J",
        "outputId": "972d7444-5898-439d-fa8b-5b37a7b83366",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 234,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_11 (Conv2D)          (None, 254, 254, 32)      896       \n",
            "                                                                 \n",
            " max_pooling2d_11 (MaxPoolin  (None, 127, 127, 32)     0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_12 (Conv2D)          (None, 125, 125, 64)      18496     \n",
            "                                                                 \n",
            " max_pooling2d_12 (MaxPoolin  (None, 62, 62, 64)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_13 (Conv2D)          (None, 60, 60, 128)       73856     \n",
            "                                                                 \n",
            " max_pooling2d_13 (MaxPoolin  (None, 30, 30, 128)      0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_14 (Conv2D)          (None, 28, 28, 256)       295168    \n",
            "                                                                 \n",
            " max_pooling2d_14 (MaxPoolin  (None, 14, 14, 256)      0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 50176)             0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 4)                 200708    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 589,124\n",
            "Trainable params: 589,124\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "N5yntHH9SmyV"
      },
      "execution_count": 235,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "\n",
        "history = model.fit(train_set, steps_per_epoch=len(train_set), \n",
        "                    validation_data=val_set, \n",
        "                    epochs=10,verbose=1)\n",
        "\n",
        "end = time.time()\n",
        "print(\"Time Taken: {:.2f} minutes\".format((end-start)/60))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "yU3kU9BWwkiU",
        "outputId": "96839f0d-f79a-44ec-8409-37014c053adc"
      },
      "execution_count": 236,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            " 57/556 [==>...........................] - ETA: 40:36 - loss: 1.3754 - accuracy: 0.3032"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-236-15a69dc29c9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m history = model.fit(train_set, steps_per_epoch=len(train_set), \n\u001b[0m\u001b[1;32m      4\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     epochs=10,verbose=1)\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1407\u001b[0m                 _r=1):\n\u001b[1;32m   1408\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1410\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2451\u001b[0m       (graph_function,\n\u001b[1;32m   2452\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2453\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2454\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1859\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1860\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1861\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1862\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    495\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    498\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save Model"
      ],
      "metadata": {
        "id": "4Rzd_AiM04SQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('history_model', 'wb') as file:\n",
        "   p.dump(history.history, file)\n",
        "\n",
        "filepath='model1.h5'\n",
        "model.save(filepath)\n",
        "filepath_model = 'model1.json'\n",
        "filepath_weights = 'weights_model.h5'\n",
        "model_json = model.to_json()\n",
        "with open(filepath_model, \"w\") as json_file:\n",
        "   json_file.write(model_json)\n",
        "\n",
        "model.save_weights('weights_model.h5')\n",
        "print(\"Saved model to disk\")"
      ],
      "metadata": {
        "id": "OcTzVu1cwnl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Model"
      ],
      "metadata": {
        "id": "NFd0vWzX0-um"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('history_model', 'rb') as file:\n",
        "   his = p.load(file)\n",
        "\n",
        "h1 = go.Scatter(y=his['val_accuracy'],\n",
        "mode=\"lines\", line=dict(\n",
        "width=2,\n",
        "color='blue'),\n",
        "name=\"val_accuracy\"\n",
        ")\n",
        "h2 = go.Scatter(y=his['val_loss'],\n",
        "mode=\"lines\", line=dict(\n",
        "width=2,\n",
        "color='red'),\n",
        "name=\"val_loss\"\n",
        ")\n",
        "\n",
        "data = [h1,h2]\n",
        "layout1 = go.Layout(title='Loss',\n",
        "xaxis=dict(title='epochs'),\n",
        "yaxis=dict(title=' '))\n",
        "fig1 = go.Figure(data, layout=layout1)\n",
        "plotly.offline.iplot(fig1, filename=\"testMNIST\")\n",
        "predict_model = load_model(filepath)\n",
        "predict_model.summary()\n",
        "with open(filepath_model, 'r') as f:\n",
        "   loaded_model_json = f.read()\n",
        "predict_model = model_from_json(loaded_model_json)\n",
        "predict_model.load_weights(filepath_weights)\n",
        "print(\"Loaded model from disk\")"
      ],
      "metadata": {
        "id": "Am-p1sNv09EN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ประเมิน"
      ],
      "metadata": {
        "id": "iPUGT8fSz6n6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (10, 3.5))\n",
        "plt.subplot(1, 2 , 1)\n",
        "plt.title('Loss')\n",
        "plt.plot(history.history['loss'], label = 'Train Loss')\n",
        "plt.plot(history.history['val_loss'], 'r', lw = 3.2, label = 'Validation loss')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "R1x7_jrZwvWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.subplot(1, 2, 2)\n",
        "plt.title('Accuracy')\n",
        "plt.plot(history.history['accuracy'], label = 'Train')\n",
        "plt.plot(history.history['val_accuracy'], 'r', lw = 3.2, label = 'Validation')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "M171vCDIw0j1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prediction"
      ],
      "metadata": {
        "id": "AYwZqP4_1MCm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from IPython.display import Image\n",
        "from io import BytesIO\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing import image \n",
        "import keras.utils as image"
      ],
      "metadata": {
        "id": "KJMyWRgt1Fi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CSC\n",
        "test_path = ('/content/drive/My Drive/lastoct/test/CSC/FQ8979 23-03-12 RE_000_cropped.png')\n",
        "img = keras.preprocessing.image.load_img(\n",
        "test_path, target_size=(img_height, img_width)\n",
        ")\n",
        "img_array = keras.preprocessing.image.img_to_array(img)\n",
        "img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
        "predictions = predict_model.predict(img_array)\n",
        "score = tf.nn.softmax(predictions[0])\n",
        "print(\"CSC\",score[0],\"Normal\",score[1],\"PCV\",score[2],\"VKH\",score[3])\n",
        "display(Image(filename=test_path,width=256, height=256))\n",
        "if score[0]==np.max(score):\n",
        "  eye = \"CSC\"\n",
        "elif score[1]==np.max(score):\n",
        "  eye = \"Normal\"\n",
        "elif score[2]==np.max(score):\n",
        "  eye = \"PCV\"\n",
        "elif score[3]==np.max(score):\n",
        "  eye = \"VKH\"\n",
        "print(\n",
        "\"AI {} มีความมั่นใจ {:.2f}%.\"\n",
        ".format(eye, 100 * np.max(score))\n",
        ")"
      ],
      "metadata": {
        "id": "ZLy56m1u1OGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normal\n",
        "test_path = ('/content/drive/My Drive/lastoct/test/Normal/CJ5901 24-01-19 RE_000_cropped.png')\n",
        "img = keras.preprocessing.image.load_img(\n",
        "test_path, target_size=(img_height, img_width)\n",
        ")\n",
        "img_array = keras.preprocessing.image.img_to_array(img)\n",
        "img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
        "predictions = predict_model.predict(img_array)\n",
        "score = tf.nn.softmax(predictions[0])\n",
        "print(\"CSC\",score[0],\"Normal\",score[1],\"PCV\",score[2],\"VKH\",score[3])\n",
        "display(Image(filename=test_path,width=256, height=256))\n",
        "if score[0]==np.max(score):\n",
        "  eye = \"CSC\"\n",
        "elif score[1]==np.max(score):\n",
        "  eye = \"Normal\"\n",
        "elif score[2]==np.max(score):\n",
        "  eye = \"PCV\"\n",
        "elif score[3]==np.max(score):\n",
        "  eye = \"VKH\"\n",
        "print(\n",
        "\"AI {} มีความมั่นใจ {:.2f}%.\"\n",
        ".format(eye, 100 * np.max(score))\n",
        ")"
      ],
      "metadata": {
        "id": "yBbH0PgM1PhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PCV\n",
        "test_path = ('/content/drive/My Drive/lastoct/test/PCV/AV0940 20-09-16 RE_001003_cropped.png')\n",
        "img = keras.preprocessing.image.load_img(\n",
        "test_path, target_size=(img_height, img_width)\n",
        ")\n",
        "img_array = keras.preprocessing.image.img_to_array(img)\n",
        "img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
        "predictions = predict_model.predict(img_array)\n",
        "score = tf.nn.softmax(predictions[0])\n",
        "print(\"CSC\",score[0],\"Normal\",score[1],\"PCV\",score[2],\"VKH\",score[3])\n",
        "display(Image(filename=test_path,width=256, height=256))\n",
        "if score[0]==np.max(score):\n",
        "  eye = \"CSC\"\n",
        "elif score[1]==np.max(score):\n",
        "  eye = \"Normal\"\n",
        "elif score[2]==np.max(score):\n",
        "  eye = \"PCV\"\n",
        "elif score[3]==np.max(score):\n",
        "  eye = \"VKH\"\n",
        "print(\n",
        "\"AI {} มีความมั่นใจ {:.2f}%.\"\n",
        ".format(eye, 100 * np.max(score))\n",
        ")"
      ],
      "metadata": {
        "id": "_HENDExC1T80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# VKH\n",
        "test_path = ('/content/drive/My Drive/lastoct/test/VKH/KH7322 18-12-20 LE_001004_cropped.png')\n",
        "img = keras.preprocessing.image.load_img(\n",
        "test_path, target_size=(img_height, img_width)\n",
        ")\n",
        "img_array = keras.preprocessing.image.img_to_array(img)\n",
        "img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
        "predictions = predict_model.predict(img_array)\n",
        "score = tf.nn.softmax(predictions[0])\n",
        "print(\"CSC\",score[0],\"Normal\",score[1],\"PCV\",score[2],\"VKH\",score[3])\n",
        "display(Image(filename=test_path,width=256, height=256))\n",
        "if score[0]==np.max(score):\n",
        "  eye = \"CSC\"\n",
        "elif score[1]==np.max(score):\n",
        "  eye = \"Normal\"\n",
        "elif score[2]==np.max(score):\n",
        "  eye = \"PCV\"\n",
        "elif score[3]==np.max(score):\n",
        "  eye = \"VKH\"\n",
        "print(\n",
        "\"AI {} มีความมั่นใจ {:.2f}%.\"\n",
        ".format(eye, 100 * np.max(score))\n",
        ")"
      ],
      "metadata": {
        "id": "tTZPw45S1Y9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing"
      ],
      "metadata": {
        "id": "OW8nrRSg1ham"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_test = []; y_pred = []\n",
        "class_names = test_set.class_names\n",
        "for image, labels in test_set:\n",
        "  for i in range(len(list(labels))):\n",
        "    img_array = keras.preprocessing.image.img_to_array(images[i])\n",
        "    X_test = tf.expand_dims(img_array, 0)\n",
        "    score = predict_model.predict(X_test)\n",
        "    if score[0][0]==np.max(score):\n",
        "      eye = \"CSC\"\n",
        "    elif score[0][1]==np.max(score):\n",
        "      eye = \"Normal\"\n",
        "    elif score[0][2]==np.max(score):\n",
        "      eye = \"PCV\"\n",
        "    elif score[0][3]==np.max(score):\n",
        "      eye = \"VKH\"\n",
        "    y_pred.append(eye)\n",
        "    y_test.append(class_names[labels[i]])"
      ],
      "metadata": {
        "id": "ovoN83nM1fuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_test)\n",
        "print(len(y_test))\n",
        "print(y_pred)\n",
        "print(len(y_pred))"
      ],
      "metadata": {
        "id": "C1do1PO81kQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Confusion Matrix"
      ],
      "metadata": {
        "id": "mZYlvlhN1uXm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "JCbXeVkM1qqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_matrix(y_test, y_pred)"
      ],
      "metadata": {
        "id": "WzMDBNQM1xII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(y_test, y_pred)"
      ],
      "metadata": {
        "id": "MerncS7P1yjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_set.class_names"
      ],
      "metadata": {
        "id": "j__dzTkP10el"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm_df = pd.DataFrame(cm,\n",
        "                     index = ['CSC', 'Normal', 'PCV', 'VKH'], \n",
        "                     columns = ['CSC', 'Normal', 'PCV', 'VKH'])"
      ],
      "metadata": {
        "id": "Jl4pv25O11_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm_df"
      ],
      "metadata": {
        "id": "JmtcWS1a15d-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "sns.heatmap(cm_df, annot=True)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.ylabel('Actual Values')\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.show()\n",
        "     "
      ],
      "metadata": {
        "id": "xHzVqcoz18AV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### หาค่า Accuracy,Recall, Precision, F1-score"
      ],
      "metadata": {
        "id": "NPT5zuRu2ELz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import f1_score"
      ],
      "metadata": {
        "id": "EKQCd_z62Aet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# accuracy: (tp + tn) / (p + n)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print('Accuracy: %f' % accuracy)"
      ],
      "metadata": {
        "id": "76a53cG52G4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# recall: tp / (tp + fn)\n",
        "recall = recall_score(y_test, y_pred, average='weighted') ### ในส่วนของ average='weighted' หนูไม่มั่นใจว่าควรใส่เป็น macro, micro หรือ weight\n",
        "print('Recall: %f' % recall)"
      ],
      "metadata": {
        "id": "Kju6g3b62I_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# precision tp / (tp + fp)\n",
        "precision = precision_score(y_test, y_pred, average='weighted') ### ในส่วนของ average='weighted' หนูไม่มั่นใจว่าควรใส่เป็น macro, micro หรือ weight\n",
        "print('Precision: %f' % precision)"
      ],
      "metadata": {
        "id": "jFAmjQbj2LAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# f1: 2 tp / (2 tp + fp + fn)\n",
        "f1 = f1_score(y_test, y_pred, average='weighted') ### ในส่วนของ average='weighted' หนูไม่มั่นใจว่าควรใส่เป็น macro, micro หรือ weight\n",
        "print('F1 score: %f' % f1)"
      ],
      "metadata": {
        "id": "CwgNIVZu2MhE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}